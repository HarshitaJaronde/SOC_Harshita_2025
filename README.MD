**This is a summary of what I have learnt during this project so far.**

## Text Processing:

- Explored regular expressions as powerful tools for pattern matching and text manipulation, enabling efficient searching, extraction, and modification of text based on specific criteria.

- used NLTK and spaCy for tokenizing text at the sentence, word, and character levels.

- Recognized the importance of text normalization for NLP tasks, including steps such as stopword removal, stemming, lemmatization, accent removal, and case normalization to simplify and standardize input data.

- Implemented part-of-speech (POS) tagging using both NLTK and spaCy.

- Created a word cloud of restaurant compliments using review data to visualize frequently mentioned positive terms.

## Neural Network:

- Gained foundational knowledge of neural networks, including the role of backpropagation in updating weights and biases.

- Explored gradient descent as an optimization technique for parameter tuning.

- Learned about the argmax and softmax functions for handling multi-class outputs.


## Pytorch:
- Understood PyTorch as a powerful library for building neural networks.

- Built a manual linear model and implemented gradient descent for weight optimization, then transitioned to using PyTorchâ€™s autograd and nn modules for more efficient model creation and training.

- Gained insight into optimizing large numbers of parameters in neural networks

## Recurrent Neural Netwroks:
- Discovered the effectiveness of RNNs for handling sequential data with variable input and output lengths, thanks to their ability to retain information across time steps.

***Noted the limitations of basic RNNs, such as the vanishing and exploding gradient problems.***

- Trained and tested an RNN model in PyTorch to classify names by country of origin using character-level sequential input, which deepened understanding of hidden layer features and softmax output interpretation.

- Experimented with improving model performance by adjusting the number of epochs and learning rate during optimization.






