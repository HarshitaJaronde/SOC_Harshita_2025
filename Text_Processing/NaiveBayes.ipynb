{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7893f8f7-96ef-4cc3-a99d-22b2f6966c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HARSHITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = list(stopwords.words('english'))\n",
    "punctuation_translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(punctuation_translator)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(s, tokenizer=None, remove_stopwords=True, remove_punctuation=True,\n",
    "                    stemmer=None, lemmatizer=None, lowercase=True, return_type='str'):\n",
    "    # Throw an error if both stemmer and lemmatizer are not None\n",
    "    if stemmer is not None and lemmatizer is not None:\n",
    "         raise ValueError(\"Stemmer and Lemmatizer cannot both be not None!\")\n",
    "\n",
    "    # Tokenization either with default tokenizer or user-specified tokenizer\n",
    "    if tokenizer is None:\n",
    "        token_list = word_tokenize(s)\n",
    "    else:\n",
    "        token_list = tokenizer.tokenize(s)\n",
    "\n",
    "    # Stem or lemmatize if needed\n",
    "    if lemmatizer is not None:\n",
    "        token_list = lemmatize_token_list(lemmatizer, token_list)\n",
    "    elif stemmer is not None:\n",
    "        token_list = stem_token_list(stemmer, token_list)\n",
    "\n",
    "    # Convert all tokens to lowercase if need\n",
    "    if lowercase:\n",
    "        token_list = [ token.lower() for token in token_list ]\n",
    "\n",
    "    # Remove all stopwords if needed\n",
    "    if remove_stopwords:\n",
    "        token_list = [ token for token in token_list if not token in nltk_stopwords ]\n",
    "\n",
    "    # Remove all punctuation marks if needed (note: also converts, e.g, \"Mr.\" to \"Mr\")\n",
    "    if remove_punctuation:\n",
    "        token_list = [ ''.join(c for c in s if c not in string.punctuation) for s in token_list ]\n",
    "        token_list = [ token for token in token_list if len(token) > 0 ] # Remove \"empty\" tokens\n",
    "\n",
    "    if return_type == 'list':\n",
    "        return token_list\n",
    "    elif return_type == 'set':\n",
    "        return set(token_list)\n",
    "    else:\n",
    "        return ' '.join(token_list)\n",
    "\n",
    "\n",
    "\n",
    "def stem_token_list(stemmer, token_list):\n",
    "    for idx, token in enumerate(token_list):\n",
    "        token_list[idx] = stemmer.stem(token)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def lemmatize_token_list(lemmatizer, token_list):\n",
    "    pos_tag_list = pos_tag(token_list)\n",
    "    for idx, (token, tag) in enumerate(pos_tag_list):\n",
    "        tag_simple = tag[0].lower() # Converts, e.g., \"VBD\" to \"c\"\n",
    "        if tag_simple in ['n', 'v', 'j']:\n",
    "            word_type = tag_simple.replace('j', 'a')\n",
    "        else:\n",
    "            word_type = 'n'\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=word_type)\n",
    "        token_list[idx] = lemmatized_token\n",
    "    return token_list\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Everything below gets only executed when the file is explicitly being run\n",
    "# and not when imported. This is useful for testing the functions.\n",
    "#\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af88957d-5819-4c7d-b3fa-ff96952abfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from nltk import bigrams\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5d356d-fc04-4069-b092-64d2abe384cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8529\n",
      "Size of test: 2133\n"
     ]
    }
   ],
   "source": [
    "# Load files using pandas\n",
    "df_sent_pos = pd.read_csv('rt-polarity.pos', sep='\\t', header=None)\n",
    "df_sent_neg = pd.read_csv('rt-polarity.neg', sep='\\t', header=None)\n",
    "\n",
    "# Create a list for all sentences and ad the sentences from both read files\n",
    "sentences = []\n",
    "sentences.extend(df_sent_neg[0].tolist())\n",
    "sentences.extend(df_sent_pos[0].tolist())\n",
    "\n",
    "# Preprocess sentences (by default, we only lowercase all letter and remove topwords and punctuation)\n",
    "sentences_preprocessed = [''] * len(sentences)\n",
    "for idx, sent in enumerate(sentences):\n",
    "    sentences_preprocessed[idx] = preprocess_text(sent)\n",
    "\n",
    "# Create a list for all lables\n",
    "polarities = []\n",
    "polarities.extend([0]*len(df_sent_neg))\n",
    "polarities.extend([1]*len(df_sent_pos))\n",
    "\n",
    "# Convert from lists to numpy arrays\n",
    "sentences = np.array(sentences_preprocessed)\n",
    "polarities = np.array(polarities)\n",
    "\n",
    "# Shuffle sentences and labels\n",
    "combined = list(zip(sentences, polarities))\n",
    "random.seed(1) # (optional)\n",
    "random.shuffle(combined)\n",
    "# split the \"zipped\" list into the two lists of sentences and labels/polarities\n",
    "sentences[:], polarities[:] = zip(*combined)\n",
    "\n",
    "# Let's go for a 80%/20% split -- you can change the value anf see its effects\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training data (the size of the dest data is also implicitly given)\n",
    "train_set_size = int(train_test_ratio * len(sentences))\n",
    "\n",
    "# Split data and labels into training and test data with respect to the size of the test data\n",
    "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
    "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "660e4bad-fafe-4fd1-abdd-b47134dc43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "log_class_priors ={}\n",
    "token_counts ={'pos':{},'neg':{}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e22190-30e5-4b27-b41f-1c4191045fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_counts(token_list):\n",
    "    token_counts={}\n",
    "    for token in token_list:\n",
    "        token_counts[token]=token_counts.get(token,0.0) +1\n",
    "    return token_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae02d166-d85d-4ae2-8c1e-07bc82a52786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lee': 1.0, 'marvelously': 1.0, 'compelling': 1.0, 'present': 1.0, 'brown': 2.0, 'catalyst': 1.0, 'struggle': 1.0, 'black': 1.0, 'manhood': 1.0, 'restrictive': 1.0, 'chaotic': 1.0, 'america': 1.0, 'sketchy': 1.0, 'nevertheless': 1.0, 'gripping': 1.0, 'portrait': 1.0, 'jim': 1.0, 'celebrated': 1.0, 'wonder': 1.0, 'spotlight': 1.0}\n"
     ]
    }
   ],
   "source": [
    "token_list = X_train[-1].split()\n",
    "print(get_token_counts(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8977e0d6-bf1f-44d7-8747-fcae20c51f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fit(X,y):\n",
    "    num_data_items = len(X)\n",
    "    log_class_priors['pos']=np.log(sum(1 for label in y if label==1)/num_data_items)\n",
    "    log_class_priors['neg']=np.log(sum(1 for label in y if label==0)/num_data_items)\n",
    "\n",
    "    for doc,label in zip(X,y):\n",
    "        polarity_class = 'pos' if label==1 else 'neg'\n",
    "        counts = get_token_counts(doc.split())\n",
    "        for token,count in counts.items():\n",
    "            vocabulary.add(token)\n",
    "            if token not in token_counts[polarity_class]:\n",
    "                token_counts[polarity_class][token]=0\n",
    "            token_counts[polarity_class][token]+=count\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6955f1c3-8ec5-4844-be17-e83bbf867a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce88fc9-8b18-4d5f-950e-b5b8ad2c4849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': -0.6930299403933299, 'neg': -0.693264434473429}\n"
     ]
    }
   ],
   "source": [
    "print(log_class_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06645f1b-030e-40a5-833f-6318f974ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors(log Probabilities): {'pos': -0.6930299403933299, 'neg': -0.693264434473429}\n",
      "Priors(probabilities): {'pos': 0.5000586235197562, 'neg': 0.4999413764802439}\n"
     ]
    }
   ],
   "source": [
    "print(\"Priors(log Probabilities): {}\".format(log_class_priors))\n",
    "print(\"Priors(probabilities): {}\".format( {k:np.exp(v) for k,v in log_class_priors.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e8ed39b-ee41-4f4e-95a5-efde79bbbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_1 = 'good'\n",
    "token_2 = 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23ee359a-9408-4bd0-ae6b-b5f2edd38c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurences of good in class POSITIVE: 156.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of occurences of {} in class POSITIVE: {}\".format(token_1,token_counts['pos'][token_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49168846-f11e-4eeb-876a-11c785f7c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurences of bad in class POSITIVE: 22.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of occurences of {} in class POSITIVE: {}\".format(token_2,token_counts['pos'][token_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26305f7d-cf06-469f-8278-0bd7798740f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurences of good in class NEGATIVE: 137.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of occurences of {} in class NEGATIVE: {}\".format(token_1,token_counts['neg'][token_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "130b99f6-c27a-4cf4-858b-22846ee622eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurences of bad in class NEGATIVE: 165.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of occurences of {} in class NEGATIVE: {}\".format(token_2,token_counts['neg'][token_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30b13888-cf42-4f56-9530-ebfd98f6a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    y_pred = []\n",
    "    \n",
    "    # Loop over all test sample and predict class label for each sample\n",
    "    for doc in X:\n",
    "        # Initialize class scores (i.e., log probablities)\n",
    "        pos_score, neg_score = 0, 0\n",
    "        # Get the number of occurrences of each token in the document\n",
    "        counts = get_token_counts(doc.split())\n",
    "        for token, _ in counts.items():\n",
    "            # Ignore unknown tokens\n",
    "            if token not in vocabulary: \n",
    "                continue\n",
    "                \n",
    "            # Add Laplace smoothing\n",
    "            log_w_given_pos = np.log( (token_counts['pos'].get(token, 0.0) + 1) / (sum(token_counts['pos'].values()) + len(vocabulary)) )\n",
    "            log_w_given_neg = np.log( (token_counts['neg'].get(token, 0.0) + 1) / (sum(token_counts['neg'].values()) + len(vocabulary)) )\n",
    " \n",
    "            # Update class scores\n",
    "            pos_score += log_w_given_pos # Since we are dealing with log probabilities here\n",
    "            neg_score += log_w_given_neg # we need to add (and not multiply) the values\n",
    " \n",
    "        # Include priors in class scores\n",
    "        pos_score += log_class_priors['pos'] # Since we are dealing with log probabilities here\n",
    "        neg_score += log_class_priors['neg'] # we need to add (and not multiply) the values\n",
    " \n",
    "        if pos_score > neg_score:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    # Return list of predicted class labels\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd606b29-7fcc-4a65-a2f5-e35266499bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(['Nice Movie','Not a good watch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8818b6a1-0cf3-4a19-b560-9761e25200c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a509f06-1e73-468d-9193-c92288201fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77      1067\n",
      "           1       0.77      0.78      0.78      1066\n",
      "\n",
      "    accuracy                           0.77      2133\n",
      "   macro avg       0.77      0.77      0.77      2133\n",
      "weighted avg       0.77      0.77      0.77      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61298878-335a-4c90-ac15-765f64c1334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.774\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ab8a3-f438-48b1-953a-aafc8778be12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy-env-30-12-2024",
   "language": "python",
   "name": "spacy-env-30-12-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
