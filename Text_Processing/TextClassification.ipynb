{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cdf77fe-8d6e-4756-b4ed-8d5244632539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87e26a3-59cf-4bff-9ecc-44dc77e588fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import KFold,cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bf2d48-a7e7-4255-a45c-8dc297b2bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46226ee0-12c9-4370-9c3c-3f2a898bc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HARSHITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = list(stopwords.words('english'))\n",
    "punctuation_translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(punctuation_translator)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(s, tokenizer=None, remove_stopwords=True, remove_punctuation=True,\n",
    "                    stemmer=None, lemmatizer=None, lowercase=True, return_type='str'):\n",
    "    # Throw an error if both stemmer and lemmatizer are not None\n",
    "    if stemmer is not None and lemmatizer is not None:\n",
    "         raise ValueError(\"Stemmer and Lemmatizer cannot both be not None!\")\n",
    "\n",
    "    # Tokenization either with default tokenizer or user-specified tokenizer\n",
    "    if tokenizer is None:\n",
    "        token_list = word_tokenize(s)\n",
    "    else:\n",
    "        token_list = tokenizer.tokenize(s)\n",
    "\n",
    "    # Stem or lemmatize if needed\n",
    "    if lemmatizer is not None:\n",
    "        token_list = lemmatize_token_list(lemmatizer, token_list)\n",
    "    elif stemmer is not None:\n",
    "        token_list = stem_token_list(stemmer, token_list)\n",
    "\n",
    "    # Convert all tokens to lowercase if need\n",
    "    if lowercase:\n",
    "        token_list = [ token.lower() for token in token_list ]\n",
    "\n",
    "    # Remove all stopwords if needed\n",
    "    if remove_stopwords:\n",
    "        token_list = [ token for token in token_list if not token in nltk_stopwords ]\n",
    "\n",
    "    # Remove all punctuation marks if needed (note: also converts, e.g, \"Mr.\" to \"Mr\")\n",
    "    if remove_punctuation:\n",
    "        token_list = [ ''.join(c for c in s if c not in string.punctuation) for s in token_list ]\n",
    "        token_list = [ token for token in token_list if len(token) > 0 ] # Remove \"empty\" tokens\n",
    "\n",
    "    if return_type == 'list':\n",
    "        return token_list\n",
    "    elif return_type == 'set':\n",
    "        return set(token_list)\n",
    "    else:\n",
    "        return ' '.join(token_list)\n",
    "\n",
    "\n",
    "\n",
    "def stem_token_list(stemmer, token_list):\n",
    "    for idx, token in enumerate(token_list):\n",
    "        token_list[idx] = stemmer.stem(token)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def lemmatize_token_list(lemmatizer, token_list):\n",
    "    pos_tag_list = pos_tag(token_list)\n",
    "    for idx, (token, tag) in enumerate(pos_tag_list):\n",
    "        tag_simple = tag[0].lower() # Converts, e.g., \"VBD\" to \"c\"\n",
    "        if tag_simple in ['n', 'v', 'j']:\n",
    "            word_type = tag_simple.replace('j', 'a')\n",
    "        else:\n",
    "            word_type = 'n'\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=word_type)\n",
    "        token_list[idx] = lemmatized_token\n",
    "    return token_list\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Everything below gets only executed when the file is explicitly being run\n",
    "# and not when imported. This is useful for testing the functions.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3113c33f-4bad-4d2b-9d71-5d57374ad684",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_pos = pd.read_csv('rt-polarity.pos', sep='\\t', header=None)\n",
    "df_sent_neg = pd.read_csv('rt-polarity.neg', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb24cc8f-5176-4253-9459-ae47f0c57ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  the rock is destined to be the 21st century's ...\n",
       "1  the gorgeously elaborate continuation of \" the...\n",
       "2                     effective but too-tepid biopic\n",
       "3  if you sometimes like to go to the movies to h...\n",
       "4  emerges as something rare , an issue movie tha..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930382c2-beb2-4884-9825-26e2c4e77977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                  simplistic , silly and tedious . \n",
       "1  it's so laddish and juvenile , only teenage bo...\n",
       "2  exploitative and largely devoid of the depth o...\n",
       "3  [garbus] discards the potential for pathologic...\n",
       "4  a visually flashy but narratively opaque and e..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df56d05d-3e07-4738-9b4b-357e5066ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentences.extend(df_sent_neg[0].tolist())\n",
    "sentences.extend(df_sent_pos[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "806ee632-d766-4336-a94e-9af859f1fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simplistic , silly and tedious . ',\n",
       " \"it's so laddish and juvenile , only teenage boys could possibly find it funny . \",\n",
       " 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . ',\n",
       " '[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . ',\n",
       " 'a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d358a31d-3eb6-4f12-9448-15b84fbe335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_preprocessed=['']*len(sentences)\n",
    "for idx,sent in enumerate(sentences):\n",
    "    sentences_preprocessed[idx]=preprocess_text(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e66e17-960a-4000-8e83-a96e86d1b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities=[]\n",
    "polarities.extend([0]*len(df_sent_neg))\n",
    "polarities.extend([1]*len(df_sent_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992df39a-1783-42aa-bfce-cb9a263e7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sentences_preprocessed)\n",
    "polarities =np.array(polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec80444-ca7a-4b9a-bead-0d34a6379cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(sentences,polarities))\n",
    "random.seed(1)\n",
    "random.shuffle(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086f6a94-f5b4-400b-8cf7-0da21b36c518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fine rousing grated family film aimed mainly little kids plenty entertainment value keep grownups squirming seats',\n",
       "  1),\n",
       " ('one look girl tight pants big tits turn stupid um nt basis entire plot', 0),\n",
       " ('s scariest guy ll see summer', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c1204a-134a-46e6-b03f-fb2a785aecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:],polarities[:]=zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6da65d-78e1-4058-9d3d-e0f86f058407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fine rousing grated family film aimed mainly little kids plenty entertainment value keep grownups squirming seats',\n",
       "       'one look girl tight pants big tits turn stupid um nt basis entire plot',\n",
       "       's scariest guy ll see summer'], dtype='<U222')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74553b9c-84d5-4aff-b7cd-e04786206feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "299b4317-1678-4304-9adb-d4195a016e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.8\n",
    "train_set_size = int(0.8*len(sentences))\n",
    "X_train,X_test = sentences[:train_set_size],sentences[train_set_size:]\n",
    "y_train,y_test = polarities[:train_set_size],polarities[train_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f653724-5f05-4f53-8cdc-8740c462f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set is 8529\n",
      "Size of testing set is 2133\n"
     ]
    }
   ],
   "source": [
    "print('Size of training set is {}'.format(len(X_train)))\n",
    "print('Size of testing set is {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8429be95-6a95-4ecd-ad84-afa9d1af62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d48eb4e-fa3d-4265-8c88-bcf73425318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "num_sentences,num_vocab = X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eac11456-19b1-4dcf-8400-abb136fad6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8529\n",
      "17732\n"
     ]
    }
   ],
   "source": [
    "print(num_sentences)\n",
    "print(num_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973bd076-5e0d-424a-b60d-dede69d644f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(solver=\"sag\").fit(X_train_tfidf,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44c59e55-7ddc-404c-978c-c4d606cd5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd98149b-d21e-48b5-821c-a752039fc4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2133, 17732)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05eebd4f-b936-4179-9d9c-43aef0215322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78      1067\n",
      "           1       0.78      0.79      0.78      1066\n",
      "\n",
      "    accuracy                           0.78      2133\n",
      "   macro avg       0.78      0.78      0.78      2133\n",
      "weighted avg       0.78      0.78      0.78      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fae204ef-0204-4276-ae83-b520f2d50112",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_list = cross_val_score(LogisticRegression(solver=\"sag\"),X_train_tfidf,y_train,cv=10,scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5dafb91-6bd4-495a-afca-912f99608a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7587007  0.74829932 0.73309609 0.74970344 0.75413712 0.75910693\n",
      " 0.75799087 0.77816492 0.7326969  0.74401914]\n"
     ]
    }
   ],
   "source": [
    "print(f1_scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a0f0d80-7681-4508-b4f7-146158c55f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7515915418363927\n",
      "0.01277275069297799\n"
     ]
    }
   ],
   "source": [
    "print(f1_scores_list.mean())\n",
    "print(f1_scores_list.std())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fd6e421-5bc9-4725-a813-9d5c61e76e2d",
   "metadata": {},
   "source": [
    "f1_scores_list.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3029c490-8f34-4d89-b437-9738dafbb7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LogisticRegression'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(LogisticRegression()).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a982a2f-dc72-403f-b3ef-edb08a82575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:LinearSVC,ngram size:1 ==> f1-score:0.749 [0:00:00.467680]\n",
      "Classifier:LinearSVC,ngram size:2 ==> f1-score:0.759 [0:00:00.890178]\n",
      "Classifier:LinearSVC,ngram size:3 ==> f1-score:0.759 [0:00:01.180116]\n",
      "Classifier:LinearSVC,ngram size:4 ==> f1-score:0.756 [0:00:01.692555]\n",
      "Classifier:LogisticRegression,ngram size:1 ==> f1-score:0.752 [0:00:00.607652]\n",
      "Classifier:LogisticRegression,ngram size:2 ==> f1-score:0.750 [0:00:01.403319]\n",
      "Classifier:LogisticRegression,ngram size:3 ==> f1-score:0.745 [0:00:02.010221]\n",
      "Classifier:LogisticRegression,ngram size:4 ==> f1-score:0.745 [0:00:02.672036]\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_classifier=None\n",
    "best_ngram_size = 0\n",
    "\n",
    "classifiers = [LinearSVC(),LogisticRegression(solver = \"sag\")]\n",
    "ngram_sizes = [1,2,3,4]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    for s in ngram_sizes:\n",
    "        start_time = datetime.now()\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,s))\n",
    "        X_train_tfidf= tfidf_vectorizer.fit_transform(X_train)\n",
    "        f1_scores_list = cross_val_score(classifier,X_train_tfidf,y_train,cv=10,scoring='f1')\n",
    "        avg_f1score = f1_scores_list.mean()\n",
    "        time_elapsed = datetime.now()-start_time\n",
    "        print('Classifier:{},ngram size:{} ==> f1-score:{:.3f} [{}]'.format(type(classifier).__name__,s,avg_f1score,time_elapsed))\n",
    "        if avg_f1score>best_score:\n",
    "            best_score = avg_f1score\n",
    "            best_classifier = classifier\n",
    "            best_ngram_size = s\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7cddfb0-4e80-468f-aa96-20ab17835d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7593098459406179\n",
      "LinearSVC\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(best_score)\n",
    "print(type(best_classifier).__name__)\n",
    "print(best_ngram_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd3dabdb-13d4-498a-b225-4b72eb5a61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,best_ngram_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8899955f-8262-48bc-82d6-048a98202314",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d6dfeb2-eaad-49e6-a3a8-4ec80318c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = best_classifier.fit(X_train_tfidf,y_train)\n",
    "y_pred = best_classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73e75417-2960-452a-b66d-686466d649aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78      1067\n",
      "           1       0.77      0.80      0.79      1066\n",
      "\n",
      "    accuracy                           0.78      2133\n",
      "   macro avg       0.78      0.78      0.78      2133\n",
      "weighted avg       0.78      0.78      0.78      2133\n",
      "\n",
      "Accuracy:0.784\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "print('Accuracy:{:.3f}'.format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93539a9d-8040-40c8-86e4-8a568d0cf7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53d42898-4a2b-46bb-afbf-a6015a86dfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ba6ea-abb4-4ca8-aabe-bd96d8001ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy-env-30-12-2024",
   "language": "python",
   "name": "spacy-env-30-12-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
